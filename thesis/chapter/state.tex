\chapter{Foundations and state of the art}
In this section, the state of the art in the field of decision-making process in robotics is examined in order to provide necessary foundations and to more specifically define the set of goals to be of concern for further analysis. Firstly, the decision-making process itself is described and classification of the decision-making architectures is given. Following, the behaviour-based approach, a particular kind of the reactive architecture is introduced and discussed in detail, serving as the theoretical foundation for the ROS Hybrid Behaviour Planner which is introduced and thoroughly described in the second part.\par
\section{Decision Making in Robotics}
The subfield of decision making in robotics is dedicated to the research of \textit{action selection mechanisms} (ASM) for artificial agents. This task involves considering the right decision making strategy, the set of actions possible to undertake for such an agent, its possibilities to perceive and store the changing state of the environment as well as the goals that the agent should aim for. The challenges of operating in dynamic, unpredictable environments and real time reaction put an additional complexity to the problem of action selection. 
\subsection{Control Architectures}
While a number of different approaches and techniques exist in this area, the most underlying classification divides them into three broad categories of control architectures and is based on the trade off between persistence and flexibility of an ASM as the differing factor \cite{attractors}.\par
The deliberative control architecture puts an emphasis on the factor of persistence. The first crucial requirement for it are advanced knowledge perceiving and storing capabilities that allow for constructing a complex, detailed model of the environment and keeping a record of past states. Furthermore, the decision making process should take the current context of an agent into account and the varying relevance of goals according to it \cite{goal-directed-decision-making}. These features in turn enable goal-oriented and persistent planning of action execution sequences. \par
The focus of the deliberative architecture on the persistence of goal pursuance is the very reason for its main flaw being lack of reactivity. The more complex the environment representation is and the more time and resources does the process of choosing an action consume, the more rigid and less flexible the resulting plan is.
While this is not problematic in well known, static and controllable environments, in dynamic, often changing or unpredictable domains this can force the agent to re-plan its actions over and over and can result in agent's inability to perform the tasks or complete the goals.\par
This lack of adaptivity in the deliberative control architecture is addressed by the reactive approach, that takes the flexibility as the main objective in the design of ASM. Such systems rely on simple reactions to the changes in the domain instead of deliberative planning \cite{attractors}. As one consequence, there is no more necessity to model and keep complex representations of the environment, instead the system can limit itself to perceive only the variables directly influencing the system's reactions saving the resources. The further benefit consists of a faster response to changes than the deliberative architecture can achieve as well as increased adaptivity and robustness in form of easier switching between executed actions in order to better fit the current situation.\par
At the same time, the reactive approach is very limited in regard to a more sophisticated goal pursuance as well as adaptivity not just in respect to changing states but also regarding new, unknown states. As the reactive architecture does not have any memory and bases on direct input-response coupling, purely reactive systems will always react in the same way to a recurrent event even if the action performed is repeatedly unsuccessful or inadequate \cite{attractors}.\par
While both deliberative and reactive control architectures focus on one extremity of the flexibility-persistence trade off, the third, hybrid approach aims to combine the benefits of both worlds while addressing their flaws. It takes form of different dual systems merging both reactive and deliberative approach and often incorporating hierarchies of actions, goals or plans.
\subsection{Behaviour Based Approach}
A particular kind of the reactive control architecture is the behaviour based approach. Introduced by P. Maes in \cite{maes} and related to as \textit{Maes's Action Selection Mechanism} (MASM), it models the problem of action selection as an emergent property of activation spreading function applied to a set of competence modules, also called behaviours. The model consists of three main components: behaviours, goals and sensors, all connected by dependencies expressed as logical conditions.\par
Sensors provide the agent with information about the current environment state. Behaviours represent actions that can be selected for as well as stopped from execution by the agent and consist of preconditions, effects and the activation level. The list of preconditions describes the desired environment state for the behaviour to become executable. The list of effects defines in turn, how the behaviour affects the environment state upon successful execution. Activation level indicates the current utility of the behaviour. Finally, goals in their simplest form consist of one or more conditions that once satisfied as an effect of a behaviour's execution and the resulting change of state, make the goal fulfilled.
\subsubsection{Behaviour Network}
Sensors, behaviours and goals connect with each other through the dependencies of their preconditions and effects creating dependent execution chains. Consideration of all such connections for a given set of behaviours, sensor inputs and goals results in a \textit{Behaviour Network}. The connections between the nodes of the network are divided into six types.\par
\textit{Successor link} between two behaviours exists when one behaviour contributes to the fulfillment of one or more preconditions of a second behaviour. For every successor link there is a \textit{predecessor link} in the opposite direction. Furthermore, there exists a \textit{conflictor link} that spreads negative activation (inhibition) to a behaviour whose effects falsify preconditions of some other behaviour. Apart from activation from behaviours further sources also exist. Activation by sensors increases the activation level of behaviours whose preconditions are fulfilled by the current environment state. Activation by goals benefits behaviors that directly contribute to goal fulfillment. Additionally, negative activation by protected goals inhibits behaviours that would make false one of goal's already satisfied conditions.\par 
\subsubsection{Activation calculation}
The process of activation calculation takes place in each execution step of an agent's operation and begins with computation of the activation from the six described sources and summing them for each behaviour. The resulting value is added to the current activation value and reflects the utility of a behaviour in the current environment state.\par
As the next step, a decay function lowers the activation level of all behaviours in the network by a certain fraction, dependent on the global decay parameter. In this way, it is assured that the overall activation level remains constant. The behaviour can be chosen for execution if it is executable, that is all of its preconditions are fulfilled and when its current activation level lies above a global activation threshold and above the activation levels of all other behaviours, that is, when an executable behaviour has the maximum utility in given execution step. If however no behaviour meets these conditions, no behaviour is chosen for execution and the activation threshold is lowered by $10\%$ to assure that achieving it will become easier as the execution time progresses until finally some behaviour's activation level will exceed that threshold.\cite{maes}.\par
Through a continuous iteration over these steps the process of action selection emerges. To better control the dynamics of the algorithm, three further global parameters are present apart from activation threshold and activation decay. The amount of activation $\phi$ controls how much activation is being injected into the network from true propositions about the environment state. The amount of activation $\gamma$ tunes the amount of activation injected into the network by goals. Finally, the amount of inhibition $\delta$ tunes how much activation is being subtracted by protected goals, that is goals that are already fulfilled but the execution of a behaviour would revert that. This set of global parameters allows for fine tuning the process of action selection in order to better fit it to a particular domain.
\subsubsection{Evaluation and improvement} \label{state:evaluation-and-improvement}
Experiments conducted in \cite{maes2} and \cite{maes} proved that for simple decision-making problem the properties of a certain goal-orientedness as well as of high adaptive capabilities hold. However, they also showed  that problems such as deadlocks where no behaviour has activation high enough to be selected for execution or loops where the same sequences of behaviors are being chosen over again, are also apparent.\par
To more thoroughly examine and evaluate the characteristics of the  MASM, a complex simulated environment basing on an animal-like action selection problem was presented by Tyrrell in \cite{tyrrell} together with a series of experiments showing that parts of Maes' activation spreading algorithm lead to unjustified bias towards some behaviours while discriminating others. \par 
In the first iteration of the evaluation, it is proven that in the activation by preconditions the division by the number of outputs causes nodes that share input from sensors with many other nodes to get a lesser amount of activation regardless of their current relevance for goal pursuance. The problem of division by the number of other nodes receiving input from the sender node also applies to activation by goal and inhibition by a protected goal.\par
Furthermore, the problem  with division by the number of inputs of a particular type (precondition, effect) of a receiving node is identified, which results in discrimination of nodes having many preconditions or effects, since activation values from all these inputs have to be high in order for the activation of a node itself to be also high in opposition to nodes with only a few inputs. Lack of the division however, leads in particular situations to an opposite problem - unjustified preference of nodes with a larger amount of inputs, since it is easier for them to achieve high activation value. As an attempt to balance these inequities Tyrrell proposes taking an average of the input value with and without division \cite{tyrrell}. \par
An evaluation of Maes's activation spreading algorithm as well as an alternative proposition can also be found in \cite{dorer}. The proposed \textit{Revised and Extended Maes's Action Selection Mechanism} (REASM) is adapted for fully continuous domains to express growing difficulty in fulfilling a goal the less the goal is already fulfilled as well as introduces the concept of situation-dependent goals that allows for differentiating the current goal relevance. Finally, the activation calculation itself is modified in the way, that activation from each goal is spread only along the strongest path from each goal eliminating the problem of preference of certain behaviours presented in \cite{tyrrell}. Successful implementation and application of these alterations in the \textit{RoboCup 99} \cite{dorer-robocup99} and \textit{RoboCup 2000} \cite{sbc++} as well as comparative experiments of MASM and REASM using the original test scenario from \cite{maes2} conducted in \cite{dorer} proved the advantage in performance of Tyrrell's proposed amended algorithm and REASM over MASM. \par
\begin{comment}
https://www.researchgate.net/profile/Jacques_Ferber/publication/228577633_An_extension_of_Maes'_action_selection_mechanism_for_animats/links/004635151e052564ed000000.pdf
https://link.springer.com/content/pdf/10.1007%2Fs11117-004-2464-2.pdf
\end{comment}
\section{ROS Hybrid Behaviour Planner}
As the decision-making is only one of many processes taking place in an autonomous artificial agent, it has to be compatible and able to interact with other components in order to build a reliable system. For this reason robotics middleware is particularly profitable for the development of agent systems as it eases the process of development, integration of new technologies and reuse of existing software components and infrastructures.\cite{middleware}. The platform gathering the biggest community as well as being widely used in academical projects and in commercial and industrial applications and in turn giving access to most recent developments also in the particular subfield of decision making is \textit{Robot Operating System} (ROS) \cite{is-ros-good}. It is an open source collection of frameworks for distributed agent software development with modularity, scalability and wide application range as main design goals. It provides among others support for hardware abstraction, inter-process communication as well as  package management, cross-language support and a common API which enable the development and sharing of libraries and tools by the community \cite{ros}. Among over 2000 publicly available packages \cite{is-ros-good} implementations of control architectures are also present. Notable here is \textit{SMACH} package basing on hierarchical and concurrent state machines \cite{smach} as well as \textit{pi\_trees} with a more task-oriented approach using behaviour trees \cite{pi_trees}.\par 
Insufficient flexibility of aforementioned implementations as well as lack of a hybrid approach is a problem that is being addressed by \textit{ROS Hybrid Behaviour Planner} (RHBP). It is an ASM that implements the hybrid control architecture and is available as a package in the ROS ecosystem. It connects concepts of both the deliberative and reactive, behaviour-based approach in a layer-based system and leverages the possibilities of this hybrid decision-making process by incorporating a coordination and adaptation mechanisms selector as well as reinforcement learning \cite{hrabia}.\par 
On the deliberate layer, the framework utilizes a symbolic planner using PDDL that is combined with the behaviour based approach  based on \cite{jung} and \cite{maes} on the reactive layer. Just like in MASM, the model consists of behaviours that represent tasks or actions that can interact with the environment using sensor objects as the information source. Their expected impact on the environment is modelled by effects. Furthermore,  goals describe desired states while wishes express the satisfaction with the current environment state as well as indicate the direction in case of a desire for a change corresponding to preconditions in the original model. Through encoding of dependencies between behaviours, sensors and goals the Behaviour Network is formed \cite{hrabia}.\par 
Unlike MASM, RHBP fully supports continuous domains allowing for the expression of continuous conditions, wishes and effects as well as the executability of a behaviour. Furthermore, the concept of a \textit{Network Behaviour} is introduced, being both a behaviour network and a single behaviour as a part of an other behaviour network. This in turn allows for building hierarchies and abstraction levels of higher and lower level behaviours. Finally, the process of activation calculation is altered in respect of MASM. In order to enable the hybrid nature of RHBP, the seventh source of activation coming from the deliberate planner is introduced and also taken into account during the aggregation of all activation sources. Thereby the behaviours included in the execution sequence generated by the planner are rewarded with the additional activation input. The division by the node's number of outgoing links is abandoned, the division by the number of incoming links is however left in place. Additionally the calculation of the indication of change by the wish and effects indicators is modified. Since both indicators span real values between $0$ and $1$, if the value of one of both wish or effect indicators would be zero while the other not, the multiplication or these two values would still result in a zero. Such values however reflect either a behaviour's wish for state change together with no actual state change happening or exactly the lack of wish for a change together with the change happening thus should result in a non-zero inhibition from both protected goals and conflicting behaviours. To overcome this problem and achieve better, steady function of activation  per behaviour over time, addition of squares of both values is chosen instead of the multiplication. The replacement of multiplication is also done for the activation by predecessors, successors and goals to unify the calculation across all the activation sources as the desired properties of the new addition in respect of multiplication  are retained. \par
While the current behaviour layer implementation of RHBP already refines and extends the original concept of behaviour network by \cite{maes}, it does not yet address the problems presented by \cite{tyrrell} as well as does not utilize improvements to the concept brought by \cite{dorer} and  described in section \ref{state:evaluation-and-improvement}. Further extension of the existing system in regard to these issues could contribute to better adaptivity and robustness.
\begin{comment}
\begin{tabular}{ l p{125mm} }
  $B$ & set of  behaviours\\
  $P$ & set of propositions about the world state\\
  $S(t)$ & set of propositions that are observed to be true at time t\\
  $G(t)$ & set of prepositions that are a goal of the agent at time t\\
  $R(t)$ & set of propositions that are a goal of the agent that has
already been achieved at time t\\
  $exec(b,t)$ & function which returns 1 if a behaviour b is executable at time t  and 0 otherwise\\
  $M(j)$ & set of behaviours that include proposition j in their precondition lists \\
  $A(j)$ & set of behaviours that include proposition j in their lists of added effects \\
  $U(j)$ & set of behaviours that include proposition j in their lists of removed effects \\
  \pi &  the mean level of activation\\
  \theta & the threshold of activation, where 8 is lowered 10\% every time no module
could be selected, and is reset to its initial value whenever a module becomes
active \\
  \phi & the amount of activation energy injected by the state per true proposition \\
  \gamma & the amount of activation energy injected by the goals per goal \\
  \delta & the amount of activation taken away by the protected goals per protected goal\\
\end{tabular}
\begin{align}
  \alpha_{precond}(b,t) &= \sum_{j \in S(t) \cap c_b} \phi \frac{1}{ |M(j)| |c_b|} \label{masm:1} \\
  a_{succ}(b,t) &=  \label{masm:2} \\
  \alpha_{pred}(b,t) &=  \label{masm:3} \\
  \alpha_{conf}(b,t) &=  \label{masm:4} \\
  \alpha_{goal}(b,t) &= \sum_{j \in G(t) \cap a_b} \gamma \frac{1}{ |A(j)| |a_b|} \label{masm:5} \\
  \alpha_{confgoal}(b,t) &= \sum_{j \in R(t) \cap d_b} \delta \frac{1}{ |U(j)| |d_b|} \label{masm:6} \\
  \alpha(b,t) &= \label{masm:aggr}
\end{align}
Gbänge: ca. 5-10 Seiten\\\\

\noindent In diesem Kapitel werden für die weitere Arbeit wichtige Begriffe eingeführt. Dabei ist darauf zu achten, nur solche Inhalte in das Grundlagenkapitel aufzunehmen, die später auch verwendet werden (Problembezogenheit). Ebenso ist auf eine ausreichend Tiefe und vollständige Darstellung der Grundlagen zu achten. Des Weiteren müssen verwandte schon vorhandene Arbeiten aus dem bearbeiteten Forschungsgebiet hier aufgeführt werden. Diese verwandten Arbeiten sind kurz zu analysieren. Wo immer möglich sind Referenzen auf vorhandene Literatur einzusetzen. d.h. nur wenn von der Literatur abweichende Definitionen und Konzepte verwendet werden, ist eine ausführliche Darstellung von Definitionen und Konzepten begründet. Die Darstellung von Definitionen und Konzepten muss unbedingt homogen und widerspruchsfrei dargestellt werden. Keinesfalls dürfen beispielsweise mehrere Definitionen des gleichen Begriffes nebeneinander gestellt werden, ohne dass eine begründete Entscheidung für die letztlich in der Arbeit verwendete Definition getroffen wird. Ein weiterer Punkt ist die Vollständigkeit der Grundlagen. So sollten alle möglichen Merkmalskombinationen abgedeckt werden, was beispielsweise mit Hilfe einer Tabelle geschehen kann.

\noindent Häufige Fehler sind:
\begin{itemize}
	\item Zuviel Grundlagen ohne Notwendigkeit für den Problemlösungsprozess
	\item	Bloßes Nebeneinanderstellen von Definitionen ohne Auswahl einer für die Arbeit verbindlichen Definition
	\item Bloßes Nebeneinanderstellen von Definitionen ohne logischen Fluss
	\item	Unstrukturiertes Aneinanderreihen von Literaturzitaten ohne Beitrag zum Problemlösungsprozess
\end{itemize}
\end{comment}